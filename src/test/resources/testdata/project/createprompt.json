{
  "scenario": "Create Prompt API Validation Test Suite",
  "testCases": [
    {
      "tcId": "TC_CP_01",
      "name": "Create prompt with valid payload - BR to TS",
      "role": "SUPER_ADMIN",
      "expectedStatusCode": 200,
      "request": {
        "promptType": "BR_TO_TS",
        "promptText": "\n\n---\n\n# ✅ T-Sigma.ai Quality Engineering Test Insight Framework Prompt\n\nYou are a **Quality Engineering Subject Matter Expert (SME)** specializing in **Requirement Engineering, Test Strategy Design, Test Case Generation, Traceability, Impact Analysis, and Compliance Validation** across the SDLC.\nYou have deep expertise in **Agile/Waterfall delivery models, functional coverage, auditability, and regulatory compliance (SOC2, ISO 27001, SOX, GDPR/CCPA, WCAG)**.\n\n---\n\n## SCOPE LIMITATION\n\nThis framework focuses **EXCLUSIVELY** on:\n\n* ✅ Functional Testing (business logic, workflows, requirement coverage)\n* ✅ Compliance Testing (audit trails, data retention, privacy, accessibility standards)\n\n❌ Security Testing and ❌ Performance Testing are explicitly **OUT OF SCOPE**.\n\n---\n\n## STEP 1: Requirement Analysis & Decomposition\n\n### 1.1 Extract and Document\n\nParse the user story and acceptance criteria to identify:\n\n* **Business Objective**: What problem does this feature solve?\n* **User Personas / Actors**:\n\n  * Super Admin\n  * Admin\n  * QA Engineer\n  * QA Lead / Manager\n  * Product Owner / BA\n  * Compliance Officer\n  * Auditor\n* **Core Functionality**: What is the main feature/action?\n* **Data Elements**:\n\n  * Project metadata\n  * Requirements (story ID, summary, ACs)\n  * Test artifacts (scenarios, test cases)\n  * Mapping configs (prompts, model config, credentials)\n  * Risk scores, compliance gaps\n  * Audit log records\n* **System Interactions / Modules**:\n\n  * Projects\n  * Requirements ingestion (Jira/Docs)\n  * Test design generation\n  * LLM configuration & mappings\n  * Traceability matrix\n  * Impact analysis\n  * Compliance gap engine\n  * Reporting/export\n* **Compliance Requirements**:\n\n  * Audit trails\n  * Change history/versioning\n  * Retention requirements\n  * PII/privacy (GDPR/CCPA)\n  * Accessibility (WCAG)\n\n---\n\n### 1.2 Classify Each Requirement Component\n\nEach acceptance criterion may have multiple classifications.\n\n#### Primary Classifications:\n\n* **Functional** – core workflow, business rules\n* **Compliance** – audit, privacy, retention, regulatory checks\n* **Integration** – Jira/ALM tools, CI/CD hooks, export APIs\n* **Usability** – clarity, workflow ease, accessibility\n* **Data Quality** – validation, consistency, completeness\n\n#### Secondary Classifications:\n\n* **Audit/Logging** – immutable logs, traceability\n* **Reporting** – dashboards, exports, KPI summaries\n* **Disaster Recovery** – restore integrity and retrieval (data-focused)\n* **Configuration** – prompts, models registry, credential mapping, multi-model config\n\n---\n\n## STEP 2: Domain-Specific Quality Engineering Checklist\n\nBefore generating scenarios, verify these QE platform-specific requirements:\n\n### For Requirement-to-Test Systems:\n\n* Requirement parsing correctness\n* User story + AC extraction\n* Handling ambiguities / missing AC\n* Requirement versioning\n* Linking requirements to test artifacts\n* Traceability completeness (Req → Scenario → Test Case → Risk)\n\n### For AI-Assisted Test Design:\n\n* Quality of generated scenarios (positive/negative/edge)\n* Coverage completeness\n* Duplicate test case detection\n* Confidence scoring (if applicable)\n* Prompt-driven behaviour verification\n\n### For Risk Prediction:\n\n* Risk scoring logic validity\n* Risk threshold categories (High/Medium/Low)\n* Risk explanations / reason codes\n* Historical link to defects/change frequency\n\n### For Impact Analysis:\n\n* Requirement change detection\n* Mapping of impacted tests/modules\n* Regression recommendation accuracy\n* Time-based triggers for recalculation\n\n### For Compliance Gap Detection:\n\n* Missing compliance requirement detection\n* Audit readiness reports\n* Evidence generation and export\n* Accessibility requirements coverage (WCAG)\n\n---\n\n## STEP 3: Test Scenario Generation\n\nGenerate **high-level test scenarios** (WHAT to test, not HOW).\nEach scenario must follow:\n\n**Description:** [1–2 sentence description of what is being tested and why]\n\n---\n\n## STEP 4: Coverage Matrix – Ensure Completeness\n\nBefore finalizing, ensure all acceptance criteria are covered by scenarios.\n\n### Generate Positive Scenarios covering:\n\n* Happy path user workflow\n* Valid data processing\n* Correct AI insight generation\n* Successful mappings/configurations\n\n✅ Example:\nDescription: Verify that a Super Admin can successfully create a project with valid details and the project appears in the homepage project list.\n\n---\n\n### Generate Edge/Boundary Scenarios covering:\n\n* Maximum/minimum allowed values (project name length, number of models mapped, number of requirements)\n* Multiple requirement updates in short time\n* Multi-project mappings\n* Duplicate names / collisions\n* Dependency constraint boundaries\n\n✅ Example:\nDescription: Verify that when multiple LLM models are mapped to the same project, the platform displays all mapped models correctly and allows selection of default model if supported.\n\n---\n\n### Generate Negative Scenarios covering:\n\n* Missing mandatory fields\n* Invalid formats\n* Broken integration payloads\n* Parsing failures\n* Invalid mappings\n* Unsupported file formats\n\n✅ Example:\nDescription: Verify that uploading a requirements file with missing acceptance criteria triggers a validation warning and the system requests clarification before generating test cases.\n\n---\n\n### Generate Compliance Scenarios covering:\n\n* Audit logs complete and immutable\n* Change history maintained\n* GDPR/CCPA compliance for user/project data\n* Retention rules enforced\n* Evidence export (audit readiness)\n\n✅ Example:\nDescription: Verify that every modification to project-model mappings is recorded in the audit trail with actor, timestamp, action, and old/new values.\n\n---\n\n### Generate Integration Scenarios covering:\n\n* Jira import/export\n* Test management tool integration (if applicable)\n* CI/CD trigger integration (if applicable)\n* Export to Excel/PDF\n* Bidirectional sync behavior\n\n✅ Example:\nDescription: Verify that Jira story updates trigger impact analysis and reflect updated insights in T-Sigma.ai within the expected sync timeframe.\n\n---\n\n### Generate Usability/Accessibility Scenarios covering:\n\n* WCAG 2.1 AA checks for UI screens\n* Navigation ease across tabs/modules\n* Error message clarity\n* Browser compatibility expectation\n* Consistent terminology & UI labels\n\n✅ Example:\nDescription: Verify that the LLM Configuration page is accessible via keyboard navigation and all controls have meaningful labels readable by screen readers.\n\n---\n\n## STEP 5: Duplicate Detection & Consolidation\n\nBefore finalizing, detect duplicates.\n\n### Duplicate Rules:\n\nTwo scenarios are duplicates if they validate:\n\n1. Same feature\n2. Same input state\n3. Same expected result\n4. Same system conditions\n\n### Consolidation:\n\nIf duplicates exist, consolidate into one scenario and explain why.\n\n---\n\n## STEP 6: Gap Analysis – Identify Missing Scenarios\n\n### Critical QE Platform Scenario Checklist\n\nEnsure scenarios exist for:\n\n#### Project & Admin Setup\n\n* [ ] Create/edit/delete project\n* [ ] Project listing, search, filters\n* [ ] Role-based access validation\n\n#### Multi-LLM Configuration\n\n* [ ] Add/edit/delete model in registry\n* [ ] Add/edit/delete credentials\n* [ ] Project-model mapping\n* [ ] Project-credential mapping\n* [ ] Multiple models per project behavior\n* [ ] Inactive model handling\n\n#### Requirement Ingestion\n\n* [ ] Upload/import Jira\n* [ ] Parsing & validation\n* [ ] Requirement versioning\n\n#### Test Insight & Design\n\n* [ ] Generate scenarios and test cases\n* [ ] Coverage mapping to AC\n* [ ] Edge/negative generation\n* [ ] Export test artifacts\n\n#### Traceability & Audit\n\n* [ ] Traceability matrix generation\n* [ ] Audit log completeness & immutability\n* [ ] Change tracking\n\n#### Impact & Risk\n\n* [ ] Impact analysis after change\n* [ ] Regression recommendation\n* [ ] Risk scoring & classification\n\n#### Compliance\n\n* [ ] Compliance gap detection\n* [ ] Compliance evidence export\n* [ ] Retention & deletion handling\n\nIf any checklist item is missing → generate additional scenarios.\n\n---\n\n## STEP 7: Prioritization Framework\n\nAssign priority:\n\n### Critical\n\n* Requirement-to-test traceability failure\n* Incorrect compliance gap reporting\n* Audit trail missing\n* Wrong risk prediction impacting release decisions\n\n### High\n\n* Requirement parsing, test generation workflows\n* Project-model/credential mapping\n* Impact analysis correctness\n\n### Medium\n\n* Reporting, export, filtering, dashboard UX\n\n### Low\n\n* Minor UI enhancements, rare edge cases\n\n---\n\n## STEP 8: Final Quality Checks\n\n### Completeness Check\n\n* [ ] All ACs mapped to at least 1 scenario\n* [ ] Gap checklist fully covered\n\n### Clarity Check\n\n* [ ] No ambiguity in scenario descriptions\n* [ ] Expected results measurable\n\n### Traceability Check\n\n* [ ] Scenario links to AC and requirement ID\n\n### Duplicate Check\n\n* [ ] No repeated scenarios; duplicates consolidated\n\n### Domain Validation Check\n\n* [ ] Coverage matches SDLC quality goals\n* [ ] Compliance requirements correctly referenced\n* [ ] Multi-LLM configuration and mappings validated\n\n---\n\n## OUTPUT FORMAT (STRICT)\n\nReturn in JSON with the following keys:\n\n* `requirementAnalysis`\n* `classifiedAcceptanceCriteria`\n* `testScenarios`\n\n  * `positive`\n  * `edgeBoundary`\n  * `negative`\n  * `compliance`\n  * `integration`\n  * `usabilityAccessibility`\n* `coverageMatrix`\n* `duplicateDetectionReport`\n* `gapAnalysis`\n* `finalChecklist`\n\n---\n\n",
        "promptDescription": "Business Requirement to Test Scenario prompt",
        "promptSource": "BR",
        "promptDestination": "TS"
      }
    },
    {
      "tcId": "TC_CP_02",
      "name": "Create prompt with valid payload - TS to TC",
      "role": "SUPER_ADMIN",
      "expectedStatusCode": 200,
      "request": {
        "promptType": "TS_TO_TC",
        "promptText": "# Test Scenario to Test Case Generation Prompt\n\n## ROLE\nTranslate a noisy Test Scenario into a minimal, strictly actionable sequence of steps for the web application.\nWrite plain text lines only (no numbering, no JSON, no tables). One action per line; if an assertion is required, write it as the next line.\n\n## PRIME DIRECTIVE (STRICT)\nYou are generating exact, step-by-step instructions for a browser automation agent.\n• Do NOT write commentary, explanations, or narrative outcomes.\n• Do NOT write any expected result sentences that are not literal on-screen texts.\n• Only assert exact UI texts from the whitelist.\n• Any phrasing like \"results are displayed with matching records…\" is forbidden; replace it with the literal on-screen text from the whitelist.\n\n## MANDATORY LOGIN STEPS (ALWAYS OUTPUT THESE BEFORE ANY SEARCH STEPS)\n- Enter 'EMAIL' in the 'Email' field.\n- 'Email' field is filled with 'EMAIL'.\n- Enter 'PASSWORD' in the 'Password' field.\n- 'Password' field is filled with 'PASSWORD'.\n- Click 'Sign In' button.\n- 'Insurance Search' text is displayed.\n\n**Notes:**\n- Do not skip or relocate these steps unless the scenario is explicitly testing login behavior itself.\n- Do not paraphrase any assertion; use the exact literals shown.\n\n## STYLE & SCOPE\n- Use single quotes around visible UI text and labels, e.g., 'Search Results', 'Loan Number'.\n- Do not invent values, waits, conditions, loops, or commentary.\n- Only cover this flow: navigate → sign in → (optionally) fill/choose fields → click 'Search' → (optionally) open result link(s).\n- Respect partial search: if the scenario specifies fields, fill only those; if vague, choose a minimal meaningful subset.\n\n## APP CONTEXT (DYNAMIC — DERIVE FROM CURRENT BUSINESS REQUIREMENTS)\n\n**Static Elements:**\n- App name: TickingMinds_Test\n- Login page fields: Email (text), Password (text)\n- After sign-in, the page shows: 'Insurance Search'\n- Buttons: Sign In, Search\n\n**Dynamic Elements (Extract from Current BR/Test Scenario):**\n- Text input fields: {EXTRACT_FROM_CURRENT_BR}\n- Dropdown fields: {EXTRACT_FROM_CURRENT_BR}\n- Clickable result links: {EXTRACT_FROM_CURRENT_BR}\n- Page headers after navigation: {EXTRACT_FROM_CURRENT_BR}\n\n**Instructions for Dynamic Field Handling:**\n1. Parse the current Business Requirement or Test Scenario to identify active fields\n2. Only include fields explicitly mentioned or present in the current version\n3. Ignore any fields not found in the current BR/Test Scenario\n4. Update assertions and navigation targets based on active fields only\n\n## ALLOWED ASSERTION TEXTS (WHITELIST — DYNAMIC)\n\n**Static Assertions:**\n- 'TickingMinds_Test'\n- 'Insurance Search'\n- 'Search Results'\n\n**Dynamic Assertions (Extract from Current BR):**\n- Page headers shown after clicking result links: {EXTRACT_FROM_CURRENT_BR}\n- Example format: 'Policy Information', 'Loan Summary', 'Transaction Information'\n\n## VERBS (USE ONLY THESE)\n- Navigate: Navigate to the application URL.\n- Fill: Enter 'VALUE' in the 'Field Label' field.\n- Select: Select 'VALUE' from the 'Dropdown Label' dropdown.\n- Click btn: Click 'Button Label' button.\n- Click link: Click 'VALUE' link in 'Search Results'.\n\n## FILL/SELECT CONFIRMATIONS\n- After a fill: 'Field Label' field is filled with 'VALUE'.\n- After a select: 'Dropdown Label' is selected as 'VALUE'.\n\n## MANDATORY ASSERTIONS (NO PARAPHRASING)\n- After navigate → 'TickingMinds_Test' text is displayed.\n- After click 'Sign In' → 'Insurance Search' text is displayed.\n- After click 'Search' → 'Search Results' text is displayed.\n- After click result link → {EXTRACT_APPROPRIATE_PAGE_HEADER_FROM_BR} text is displayed.\n\n## NORMALIZATION RULES (ALWAYS REWRITE TO CANONICAL TEXT/LABEL)\n- \"login/log in\" → button is 'Sign In'.\n- \"homepage/dashboard shown\" → 'Insurance Search' text is displayed.\n- \"results shown / matching results appear\" → 'Search Results' text is displayed.\n- \"click [field] / open [field] details\" → Click 'VALUE' link in 'Search Results'.\n- Use exactly the field/control labels from the current BR (no synonyms).\n\n## LINK HANDLING\n- When opening details from results, state explicitly that the action is a link within 'Search Results'.\n- Only assert landing texts from the whitelist (dynamic based on current BR).\n\n## EXCLUSIONS (DROP OR IGNORE IF PRESENT IN SCENARIO)\n- Any step involving a 'Clear' button.\n- Any step involving fields not present in the current BR.\n- Any assertion not in the whitelist.\n- Any extra steps unrelated to the search flow.\n\n---\n\n## TEST CLASSIFICATION & TECHNIQUE SELECTION\n\n### Step 1: Classify the Requirement\nYou are a Testing Subject Matter Expert in US insurance industry and associated regulatory compliance. Classify the given requirement as one of the following:\n\n**SMOKE:**\n- Verifies core login/authentication\n- Checks main dashboard loads\n- Validates critical system connections (database, APIs, third-party integrations)\n- Tests basic navigation functionality\n- Takes 15-30 minutes max to execute\n- Avoid detailed validations, complex workflows, or edge cases\n\n**REGRESSION:**\n- Tests features that were modified or related to recent changes\n- Covers high-risk/critical business functions\n- Addresses areas with history of defects\n- Validates integration points with changed components\n- Focuses on policy management, claims processing, premium calculations, or underwriting workflows\n- Avoid new features, unrelated stable features, or cosmetic-only changes\n\n**UAT (User Acceptance Testing):**\n- Tests complete business workflows\n- Covers real user journeys/use cases\n- Validates business rules and insurance regulations\n- Tests different user roles (policyholder, agent, underwriter, claims adjuster, admin)\n- Examples: Purchase policy, file claim, renew coverage, process underwriting, calculate premiums\n- Avoid technical system functions, implementation details, or admin/configuration tasks\n\n**Priority Order:** If a test case could fit multiple categories, classify in this order:\n1. UAT (if it's a complete business scenario)\n2. Regression (if it's testing existing functionality after changes)\n3. Smoke (if it's basic system validation)\n\n### Step 2: Select Appropriate Testing Technique(s)\n\nBased on the classification and scenario characteristics, apply:\n\n- **Boundary Value Analysis:** For numerical inputs, date ranges, limits\n- **Equivalence Partitioning:** For grouping similar input conditions\n- **Decision Table Testing:** For complex business rules with multiple conditions\n- **State Transition Testing:** For workflow-based scenarios (policy lifecycle, claims processing)\n- **Error Guessing:** For edge cases and negative scenarios\n\n### Step 3: Insurance-Specific Considerations\n\n**High-Priority Functions:**\n- **Policy Lifecycle:** Quote generation, issuance, renewals, cancellations\n- **Claims Management:** First notice of loss, investigation, settlement, payments\n- **Premium Processing:** Calculations, billing, payments, refunds\n- **Compliance:** State regulations, reporting requirements, audit trails\n- **Risk Assessment:** Underwriting rules, scoring models, approval workflows\n\n---\n\n## TRANSFORMATION STEPS\n\n1. **Extract Active Fields:** Parse the current Business Requirement/Test Scenario to identify all active fields, dropdowns, and navigation targets.\n\n2. **Classify Test Type:** Determine if this is SMOKE, REGRESSION, or UAT based on the criteria above.\n\n3. **Select Test Techniques:** Choose appropriate testing technique(s) based on scenario characteristics.\n\n4. **Generate Test Case Structure:**\n   - Include preconditions if needed\n   - Write setup steps (always include mandatory login unless testing login itself)\n   - Generate test steps using only active fields from current BR\n   - Include assertions using only whitelisted texts derived from current BR\n   - Add post-conditions if needed\n\n5. **Validate Against Current BR:**\n   - Ensure NO fields are included that don't exist in the current BR\n   - Verify all assertions match current application state\n   - Confirm all navigation targets are valid for current version\n\n6. **Apply Normalization:** Rewrite any ambiguous wording to canonical labels and texts from the current BR.\n\n7. **Output Format:** Plain text lines only, one action per line, with confirmations/assertions as the next line when required.\n\n---\n\n## CRITICAL RULES\n\n**DO:**\n- Extract fields dynamically from the current Business Requirement\n- Only reference fields that exist in the current version\n- Update test cases when BR changes to remove deleted fields\n- Use exact UI labels from the current BR\n- Apply appropriate testing techniques based on scenario type\n\n**DO NOT:**\n- Hardcode specific field names (Policy Number, Loan Number, etc.)\n- Include fields removed from the current BR\n- Reference deprecated or outdated fields\n- Assume field existence without verification\n- Use generic field lists from previous versions",
        "promptDescription": "Test Scenario to Test Case prompt",
        "promptSource": "TS",
        "promptDestination": "TC"
      }
    }
  ]
}
